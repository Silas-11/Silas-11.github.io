<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="深度学习&lt;一&gt;, Silas">
    <meta name="description" content="深度学习&amp;lt;一&amp;gt;1 基本概念1.1 神经网络组成？神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，我们先从最简单的神经网络说起。
感知机
多层感知机中的特征神经元模型称为感知机，由Frank Rosenbla">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>深度学习&lt;一&gt; | Silas</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Silas</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Silas</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">深度学习&lt;一&gt;</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">深度学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                深度学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-02-08
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="深度学习-lt-一-gt"><a href="#深度学习-lt-一-gt" class="headerlink" title="深度学习&lt;一&gt;"></a>深度学习&lt;一&gt;</h1><h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1 基本概念"></a>1 基本概念</h2><h3 id="1-1-神经网络组成？"><a href="#1-1-神经网络组成？" class="headerlink" title="1.1 神经网络组成？"></a>1.1 神经网络组成？</h3><p>神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，我们先从最简单的神经网络说起。</p>
<p><strong>感知机</strong></p>
<p>多层感知机中的特征神经元模型称为感知机，由<em>Frank Rosenblatt</em>于1957年发明。</p>
<p>简单的感知机如下图所示：</p>
<p><img src="https://s2.loli.net/2023/09/25/r3PK8BdgofAjTvE.png" alt=""></p>
<p>其中$x_1$，$x_2$，$x_3$为感知机的输入，其输出为：</p>
<script type="math/tex; mode=display">
output = \left\{
\begin{aligned}
0, \quad if \ \ \sum_i w_i x_i \leqslant threshold \\
1, \quad if \ \ \sum_i w_i x_i > threshold
\end{aligned}
\right.</script><p>假如把感知机想象成一个加权投票机制，比如 3 位评委给一个歌手打分，打分分别为$ 4 $分、$1$ 分、$-3 $分，这$ 3$ 位评分的权重分别是 $1、3、2$，则该歌手最终得分为 $4 \times 1 + 1 \times 3 + (-3) \times 2 = 1$ 。按照比赛规则，选取的 $threshold$ 为 $3$，说明只有歌手的综合评分大于$ 3$ 时，才可顺利晋级。对照感知机，该选手被淘汰，因为：</p>
<script type="math/tex; mode=display">
\sum_i w_i x_i < threshold=3, output = 0</script><p>用 $-b$  代替 $threshold$，输出变为：</p>
<script type="math/tex; mode=display">
output = \left\{
\begin{aligned}
0, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b \leqslant 0 \\
1, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b > 0
\end{aligned}
\right.</script><p>设置合适的  $\boldsymbol{x}$  和  $b$ ，一个简单的感知机单元的与非门表示如下：</p>
<p><img src="https://s2.loli.net/2023/09/25/JXb82rHGi7lORMk.png" alt=""></p>
<p>当输入为 $0$，$1$ 时，感知机输出为 $ 0 \times (-2) + 1 \times (-2) + 3 = 1$。</p>
<p>复杂一些的感知机由简单的感知机单元组合而成：</p>
<p><img src="https://s2.loli.net/2023/09/25/PRTXjgI1csvOCao.png" alt=""></p>
<p><strong>多层感知机</strong></p>
<p>多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第 $ i $ 层的每个神经元和第 $ i-1 $ 层的每个神经元都有连接。</p>
<p><img src="https://s2.loli.net/2023/09/25/i6HJPp2gDyWEezA.png" alt=""></p>
<p>输出层可以不止有$ 1$ 个神经元。隐藏层可以只有$ 1$ 层，也可以有多层。输出层为多个神经元的神经网络例如下图所示：</p>
<p><img src="https://s2.loli.net/2023/09/25/YhDzIg8jrfRiy1Z.png" alt=""></p>
<h3 id="1-2-神经网络有哪些常用模型结构？"><a href="#1-2-神经网络有哪些常用模型结构？" class="headerlink" title="1.2 神经网络有哪些常用模型结构？"></a>1.2 神经网络有哪些常用模型结构？</h3><p>下图包含了大部分常用的模型：</p>
<p><img src="J:/DeepLearning-500-questions-master/ch03_深度学习基础/img/ch3/3-7.jpg" alt=""></p>
<h3 id="1-3-如何选择深度学习开发平台？"><a href="#1-3-如何选择深度学习开发平台？" class="headerlink" title="1.3 如何选择深度学习开发平台？"></a>1.3 如何选择深度学习开发平台？</h3><p>​    现有的深度学习开源平台主要有 Caffe, PyTorch, MXNet, CNTK, Theano, TensorFlow, Keras, fastai等。那如何选择一个适合自己的平台呢，下面列出一些衡量做参考。</p>
<p><strong>参考1：与现有编程平台、技能整合的难易程度</strong></p>
<p>​    主要是前期积累的开发经验和资源，比如编程语言，前期数据集存储格式等。</p>
<p><strong>参考2: 与相关机器学习、数据处理生态整合的紧密程度</strong></p>
<p>​    深度学习研究离不开各种数据处理、可视化、统计推断等软件包。考虑建模之前，是否具有方便的数据预处理工具？建模之后，是否具有方便的工具进行可视化、统计推断、数据分析。  </p>
<p><strong>参考3：对数据量及硬件的要求和支持</strong></p>
<p>​    深度学习在不同应用场景的数据量是不一样的，这也就导致我们可能需要考虑分布式计算、多GPU计算的问题。例如，对计算机图像处理研究的人员往往需要将图像文件和计算任务分部到多台计算机节点上进行执行。当下每个深度学习平台都在快速发展，每个平台对分布式计算等场景的支持也在不断演进。</p>
<p><strong>参考4：深度学习平台的成熟程度</strong></p>
<p>​    成熟程度的考量是一个比较主观的考量因素，这些因素可包括：社区的活跃程度；是否容易和开发人员进行交流；当前应用的势头。</p>
<p><strong>参考5：平台利用是否多样性？</strong></p>
<p>​    有些平台是专门为深度学习研究和应用进行开发的，有些平台对分布式计算、GPU 等构架都有强大的优化，能否用这些平台/软件做其他事情？比如有些深度学习软件是可以用来求解二次型优化；有些深度学习平台很容易被扩展，被运用在强化学习的应用中。</p>
<h3 id="1-4-为什么使用深层表示"><a href="#1-4-为什么使用深层表示" class="headerlink" title="1.4 为什么使用深层表示?"></a>1.4 为什么使用深层表示?</h3><ol>
<li>深度神经网络是一种特征递进式的学习算法，浅层的神经元直接从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而深层的特征则基于已学习到的浅层特征继续学习更高级的特征，从计算机的角度学习深层的语义信息。</li>
<li>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</li>
</ol>
<h3 id="1-5-为什么深层神经网络难以训练？"><a href="#1-5-为什么深层神经网络难以训练？" class="headerlink" title="1.5 为什么深层神经网络难以训练？"></a>1.5 为什么深层神经网络难以训练？</h3><ol>
<li><p>梯度消失</p>
<pre><code>梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。
</code></pre><p>​    梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率：</p>
</li>
</ol>
<p><img src="https://s2.loli.net/2023/09/25/ZEWNP4cUaqj26sy.png" alt=""></p>
<ol>
<li><p>梯度爆炸</p>
<pre><code>在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为$NaN$值，再也无法更新。
</code></pre></li>
<li><p>权重矩阵的退化导致模型的有效自由度减少。</p>
<p>​    参数空间中学习的退化速度减慢，导致减少了模型的有效维数，网络的可用自由度对学习中梯度范数的贡献不均衡，随着相乘矩阵的数量（即网络深度）的增加，矩阵的乘积变得越来越退化。在有硬饱和边界的非线性网络中（例如 ReLU 网络），随着深度增加，退化过程会变得越来越快。Duvenaud等人2014年的论文里展示了关于该退化过程的可视化：</p>
</li>
</ol>
<p><img src="https://s2.loli.net/2023/09/25/qySvnODQmhUui1C.jpg" alt=""></p>
<p>随着深度的增加，输入空间（左上角所示）会在输入空间中的每个点处被扭曲成越来越细的单丝，只有一个与细丝正交的方向影响网络的响应。沿着这个方向，网络实际上对变化变得非常敏感。</p>
<h3 id="1-6-深度学习和机器学习有什么不同？"><a href="#1-6-深度学习和机器学习有什么不同？" class="headerlink" title="1.6 深度学习和机器学习有什么不同？"></a>1.6 深度学习和机器学习有什么不同？</h3><p>​    <strong>机器学习</strong>：利用计算机、概率论、统计学等知识，输入数据，让计算机学会新知识。机器学习的过程，就是训练数据去优化目标函数。</p>
<p>​    <strong>深度学习</strong>：是一种特殊的机器学习，具有强大的能力和灵活性。它通过学习将世界表示为嵌套的层次结构，每个表示都与更简单的特征相关，而抽象的表示则用于计算更抽象的表示。</p>
<p>​    传统的机器学习需要定义一些手工特征，从而有目的的去提取目标信息， 非常依赖任务的特异性以及设计特征的专家经验。而深度学习可以从大数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，不依赖人工的特征工程，这也是深度学习在大数据时代受欢迎的一大原因。</p>
<p><img src="https://s2.loli.net/2023/09/25/1wIaQtGi75A4EnU.png" alt=""></p>
<p><img src="https://s2.loli.net/2023/09/25/JmIwy3hDGHcp8bY.jpg" alt=""></p>
<h2 id="2-网络操作与计算"><a href="#2-网络操作与计算" class="headerlink" title="2 网络操作与计算"></a>2 网络操作与计算</h2><h3 id="2-1-前向传播与反向传播？"><a href="#2-1-前向传播与反向传播？" class="headerlink" title="2.1 前向传播与反向传播？"></a>2.1 前向传播与反向传播？</h3><p>神经网络的计算主要有两种：前向传播（foward propagation, FP）作用于每一层的输入，通过逐层计算得到输出结果；反向传播（backward propagation, BP）作用于网络的输出，通过计算梯度由深到浅更新网络参数。</p>
<p><strong>前向传播</strong></p>
<p><img src="https://s2.loli.net/2023/09/25/HReUyO6JZE7Xlus.png" alt=""></p>
<p>假设上一层结点 $ i,j,k,… $ 等一些结点与本层的结点 $ w $ 有连接，那么结点 $ w $ 的值怎么算呢？就是通过上一层的 $ i,j,k,… $ 等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如 $ReLu$，$sigmoid$ 等函数，最后得到的结果就是本层结点 $ w $ 的输出。 </p>
<p>最终不断的通过这种方法一层层的运算，得到输出层结果。</p>
<p><strong>反向传播</strong></p>
<p><img src="https://s2.loli.net/2023/09/25/K7oErv5C9JYhAHs.png" alt=""></p>
<p>由于我们前向传播最终得到的结果，以分类为例，最终总是有误差的，那么怎么减少误差呢，当前应用广泛的一个算法就是梯度下降算法，但是求梯度就要求偏导数，下面以图中字母为例讲解一下：</p>
<p>设最终误差为 $ E $且输出层的激活函数为线性激活函数，对于输出那么 $ E $ 对于输出节点 $ y_l $ 的偏导数是 $ y_l - t_l $，其中 $ t_l $ 是真实值，$ \frac{\partial y_l}{\partial z_l} $ 是指上面提到的激活函数，$ z_l $ 是上面提到的加权和，那么这一层的 $ E $ 对于 $ z_l $ 的偏导数为 $ \frac{\partial E}{\partial z_l} = \frac{\partial E}{\partial y_l} \frac{\partial y_l}{\partial z_l} $。同理，下一层也是这么计算，只不过 $ \frac{\partial E}{\partial y_k} $ 计算方法变了，一直反向传播到输入层，最后有 $ \frac{\partial E}{\partial x_i} = \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial z_j} $，且 $ \frac{\partial z_j}{\partial x_i} = w_i j $。然后调整这些过程中的权值，再不断进行前向传播和反向传播的过程，最终得到一个比较好的结果。</p>
<h3 id="2-2-如何计算神经网络的输出？"><a href="#2-2-如何计算神经网络的输出？" class="headerlink" title="2.2 如何计算神经网络的输出？"></a>2.2 如何计算神经网络的输出？</h3><p><img src="https://s2.loli.net/2023/09/25/lKqa6XAuSy25PzR.png" alt=""></p>
<p>如上图，输入层有三个节点，我们将其依次编号为 1、2、3；隐藏层的 4 个节点，编号依次为 4、5、6、7；最后输出层的两个节点编号为 8、9。比如，隐藏层的节点 4，它和输入层的三个节点 1、2、3 之间都有连接，其连接上的权重分别为是 $ w_{41}, w_{42}, w_{43} $。</p>
<p>为了计算节点 4 的输出值，我们必须先得到其所有上游节点（也就是节点 1、2、3）的输出值。节点 1、2、3 是输入层的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点 1、2、3 的输出值分别是 $ x_1, x_2, x_3 $。</p>
<script type="math/tex; mode=display">
a_4 = \sigma(w^T \cdot a) = \sigma(w_{41}x_4 + w_{42}x_2 + w_{43}a_3 + w_{4b})</script><p>其中 $ w_{4b} $ 是节点 4 的偏置项。</p>
<p>同样，我们可以继续计算出节点 5、6、7 的输出值 $ a_5, a_6, a_7 $。</p>
<p>计算输出层的节点 8 的输出值 $ y_1 $：</p>
<script type="math/tex; mode=display">
y_1 = \sigma(w^T \cdot a) = \sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b})</script><p>其中 $ w_{8b} $ 是节点 8 的偏置项。</p>
<p>同理，我们还可以计算出 $ y_2 $。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量 $ x_1, x_2, x_3, x_4 $ 时，神经网络的输出向量 $ y_1, y_2 $ 。这里我们也看到，输出向量的维度和输出层神经元个数相同。</p>
<h3 id="2-3-如何计算卷积神经网络输出值？"><a href="#2-3-如何计算卷积神经网络输出值？" class="headerlink" title="2.3 如何计算卷积神经网络输出值？"></a>2.3 如何计算卷积神经网络输出值？</h3><p>假设有一个 5*5 的图像，使用一个 3*3 的 filter 进行卷积，想得到一个 3*3 的 Feature Map，如下所示：</p>
<p><img src="https://s2.loli.net/2023/09/25/CZqwp37bvBLi1Gk.png" alt=""></p>
<p>$ x_{i,j} $ 表示图像第  $ i $ 行第 $ j $ 列元素。$ w_{m,n} $ 表示 filter​ 第 $ m $ 行第 $ n $ 列权重。 $ w_b $ 表示 $filter$ 的偏置项。 表$a_i,_j$示 feature map 第 $ i$ 行第 $ j $ 列元素。 $f$ 表示激活函数，这里以$ ReLU$ 函数为例。</p>
<p>卷积计算公式如下：</p>
<script type="math/tex; mode=display">
a_{i,j} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b )</script><p>当步长为 $1$ 时，计算 feature map 元素 $ a_{0,0} $ 如下：</p>
<script type="math/tex; mode=display">
a_{0,0} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{0+m, 0+n} + w_b )

= relu(w_{0,0} x_{0,0} + w_{0,1} x_{0,1} + w_{0,2} x_{0,2} + w_{1,0} x_{1,0} + \\w_{1,1} x_{1,1} + w_{1,2} x_{1,2} + w_{2,0} x_{2,0} + w_{2,1} x_{2,1} + w_{2,2} x_{2,2}) \\

= 1 + 0 + 1 + 0 + 1 + 0 + 0 + 0 + 1 \\

= 4</script><p>其计算过程图示如下：</p>
<p><img src="https://s2.loli.net/2023/09/25/TPIanGwb2ryCXRv.png" alt=""></p>
<p>以此类推，计算出全部的Feature Map。</p>
<p><img src="https://s2.loli.net/2023/09/25/qbKF4WTIsoRcHuO.png" alt=""></p>
<p>当步幅为 2 时，Feature Map计算如下</p>
<p><img src="https://s2.loli.net/2023/09/25/y6uREi7rx3nN9Jt.png" alt=""></p>
<p>注：图像大小、步幅和卷积后的Feature Map大小是有关系的。它们满足下面的关系：</p>
<script type="math/tex; mode=display">
W_2 = (W_1 - F + 2P)/S + 1\\
H_2 = (H_1 - F + 2P)/S + 1</script><p>​    其中 $ W_2 $， 是卷积后 Feature Map 的宽度；$ W_1 $ 是卷积前图像的宽度；$ F $ 是 filter 的宽度；$ P $ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 $0$，如果 $P$ 的值是 $1$，那么就补 $1$ 圈 $0$；$S$ 是步幅；$ H_2 $ 卷积后 Feature Map 的高度；$ H_1 $ 是卷积前图像的宽度。</p>
<p>​    举例：假设图像宽度 $ W_1 = 5 $，filter 宽度 $ F=3 $，Zero Padding $ P=0 $，步幅 $ S=2 $，$ Z $ 则</p>
<script type="math/tex; mode=display">
W_2 = (W_1 - F + 2P)/S + 1

= (5-3+0)/2 + 1

= 2</script><p>​    说明 Feature Map 宽度是2。同样，我们也可以计算出 Feature Map 高度也是 2。</p>
<p>如果卷积前的图像深度为 $ D $，那么相应的 filter 的深度也必须为 $ D $。深度大于 1 的卷积计算公式：</p>
<script type="math/tex; mode=display">
a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b)</script><p>​    其中，$ D $ 是深度；$ F $ 是 filter 的大小；$ w_{d,m,n} $ 表示 filter 的第 $ d $ 层第 $ m $ 行第 $ n $ 列权重；$ a_{d,i,j} $ 表示 feature map 的第 $ d $ 层第 $ i $ 行第 $ j $ 列像素；其它的符号含义前面相同，不再赘述。</p>
<p>​    每个卷积层可以有多个 filter。每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。卷积后 Feature Map 的深度(个数)和卷积层的 filter 个数相同。下面的图示显示了包含两个 filter 的卷积层的计算。$7<em>7</em>3$ 输入，经过两个 $3<em>3</em>3$ filter 的卷积(步幅为 $2$)，得到了 $3<em>3</em>2$ 的输出。图中的 Zero padding 是 $1$，也就是在输入元素的周围补了一圈 $0$。</p>
<p><img src="J:/DeepLearning-500-questions-master/ch03_深度学习基础/img/ch3/3.2.3.6.png" alt=""></p>
<p>​    以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个 $ 3 <em> 3 </em> 3 $ 的 fitler 的卷积层来说，其参数数量仅有 $ (3 <em> 3 </em> 3+1) * 2 = 56 $ 个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。</p>
<h3 id="2-4-如何计算-Pooling-层输出值输出值？"><a href="#2-4-如何计算-Pooling-层输出值输出值？" class="headerlink" title="2.4 如何计算 Pooling 层输出值输出值？"></a>2.4 如何计算 Pooling 层输出值输出值？</h3><p>​    Pooling 层主要的作用是下采样，通过去掉 Feature Map 中不重要的样本，进一步减少参数数量。Pooling 的方法很多，最常用的是 Max Pooling。Max Pooling 实际上就是在 n*n 的样本中取最大值，作为采样后的样本值。下图是 2*2 max pooling：</p>
<p><img src="https://s2.loli.net/2023/09/25/6Pj7B3zCLWJsZkT.png" alt=""></p>
<p>​    除了 Max Pooing 之外，常用的还有 Average Pooling ——取各样本的平均值。<br>​    对于深度为 $ D $ 的 Feature Map，各层独立做 Pooling，因此 Pooling 后的深度仍然为 $ D $。</p>
<h3 id="2-5-实例理解反向传播"><a href="#2-5-实例理解反向传播" class="headerlink" title="2.5 实例理解反向传播"></a>2.5 实例理解反向传播</h3><p>​    一个典型的三层神经网络如下所示：</p>
<p><img src="https://s2.loli.net/2023/09/25/N6i1WHkvVs8zmRI.png" alt=""></p>
<p>​    其中 Layer $ L_1 $ 是输入层，Layer $ L_2 $ 是隐含层，Layer $ L_3 $ 是输出层。</p>
<p>​    假设输入数据集为 $ D={x_1, x_2, …, x_n} $，输出数据集为 $ y_1, y_2, …, y_n $。</p>
<p>​    如果输入和输出是一样，即为自编码模型。如果原始数据经过映射，会得到不同于输入的输出。</p>
<p>假设有如下的网络层：</p>
<p><img src="https://s2.loli.net/2023/09/25/3fqaSEPyctekUgW.png" alt=""></p>
<p>​    输入层包含神经元 $ i_1, i_2 $，偏置 $ b_1 $；隐含层包含神经元 $ h_1, h_2 $，偏置 $ b_2 $，输出层为  $ o_1, o_2 $，$ w_i $ 为层与层之间连接的权重，激活函数为 $sigmoid$ 函数。对以上参数取初始值，如下图所示：</p>
<p><img src="J:/DeepLearning-500-questions-master/ch03_深度学习基础/img/ch3/3.2.5.3.png" alt=""></p>
<p>其中：</p>
<ul>
<li>输入数据 $ i1=0.05, i2 = 0.10 $</li>
<li>输出数据 $ o1=0.01, o2=0.99 $;</li>
<li>初始权重 $ w1=0.15, w2=0.20, w3=0.25,w4=0.30, w5=0.40, w6=0.45, w7=0.50, w8=0.55 $</li>
<li>目标：给出输入数据 $ i1,i2 $ ( $0.05$和$0.10$ )，使输出尽可能与原始输出 $ o1,o2 $，( $0.01$和$0.99$)接近。</li>
</ul>
<p><strong>前向传播</strong></p>
<ol>
<li>输入层 —&gt; 输出层</li>
</ol>
<p>计算神经元 $ h1 $ 的输入加权和：</p>
<script type="math/tex; mode=display">
net_{h1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1\\

net_{h1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1 = 0.3775</script><p>神经元 $ h1 $ 的输出 $ o1 $ ：（此处用到激活函数为 sigmoid 函数）：</p>
<script type="math/tex; mode=display">
out_{h1} = \frac{1}{1 + e^{-net_{h1}}} = \frac{1}{1 + e^{-0.3775}} = 0.593269992</script><p>同理，可计算出神经元 $ h2 $ 的输出 $ o1 $：</p>
<script type="math/tex; mode=display">
out_{h2} = 0.596884378</script><ol>
<li>隐含层—&gt;输出层：  　　</li>
</ol>
<p>计算输出层神经元 $ o1 $ 和 $ o2 $ 的值：</p>
<script type="math/tex; mode=display">
net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1</script><script type="math/tex; mode=display">
net_{o1} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1 = 1.105905967</script><script type="math/tex; mode=display">
out_{o1} = \frac{1}{1 + e^{-net_{o1}}} = \frac{1}{1 + e^{1.105905967}} = 0.75136079</script><p>这样前向传播的过程就结束了，我们得到输出值为 $ [0.75136079 ,  0.772928465] $，与实际值 $ [0.01 , 0.99] $ 相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。</p>
<p><strong>反向传播 </strong></p>
<p>​    1.计算总误差</p>
<p>总误差：(这里使用Square Error)</p>
<script type="math/tex; mode=display">
E_{total} = \sum \frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $ o1 $ 和 $ o2 $ 的误差，总误差为两者之和：</p>
<p>$E_{o1} = \frac{1}{2}(target_{o1} - out_{o1})^2<br>= \frac{1}{2}(0.01 - 0.75136507)^2 = 0.274811083$.</p>
<p>$E_{o2} = 0.023560026$.</p>
<p>$E_{total} = E_{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109$.</p>
<p>​    2.隐含层 —&gt; 输出层的权值更新：</p>
<p>以权重参数 $ w5 $ 为例，如果我们想知道 $ w5 $ 对整体误差产生了多少影响，可以用整体误差对 $ w5 $ 求偏导求出：（链式法则）</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w5} = \frac{\partial E_{total}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial w5}</script><p>下面的图可以更直观的看清楚误差是怎样反向传播的：</p>
<p><img src="https://s2.loli.net/2023/09/25/GEITpaxJchUojbs.png" alt=""></p>
<h3 id="2-6-神经网络更“深”有什么意义？"><a href="#2-6-神经网络更“深”有什么意义？" class="headerlink" title="2.6 神经网络更“深”有什么意义？"></a>2.6 神经网络更“深”有什么意义？</h3><p>前提：在一定范围内。</p>
<ul>
<li>在神经元数量相同的情况下，深层网络结构具有更大容量，分层组合带来的是指数级的表达空间，能够组合成更多不同类型的子结构，这样可以更容易地学习和表示各种特征。</li>
<li>隐藏层增加则意味着由激活函数带来的非线性变换的嵌套层数更多，就能构造更复杂的映射关系。</li>
</ul>
<h2 id="3-超参数"><a href="#3-超参数" class="headerlink" title="3 超参数"></a>3 超参数</h2><h3 id="3-1-什么是超参数？"><a href="#3-1-什么是超参数？" class="headerlink" title="3.1 什么是超参数？"></a>3.1 什么是超参数？</h3><p>​    <strong>超参数</strong> : 在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</p>
<p>​    超参数通常存在于：</p>
<pre><code>1.  定义关于模型的更高层次的概念，如复杂性或学习能力。
2.  不能直接从标准模型培训过程中的数据中学习，需要预先定义。
3.  可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定
</code></pre><p>​    超参数具体来讲比如算法中的学习率（learning rate）、梯度下降法迭代的数量（iterations）、隐藏层数目（hidden layers）、隐藏层单元数目、激活函数（ activation function）都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。</p>
<h3 id="3-2-如何寻找超参数的最优值？"><a href="#3-2-如何寻找超参数的最优值？" class="headerlink" title="3.2 如何寻找超参数的最优值？"></a>3.2 如何寻找超参数的最优值？</h3><p>​    在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有：</p>
<ol>
<li><p>猜测和检查：根据经验或直觉，选择参数，一直迭代。</p>
</li>
<li><p>网格搜索：让计算机尝试在一定范围内均匀分布的一组值。</p>
</li>
<li><p>随机搜索：让计算机随机挑选一组值。</p>
</li>
<li><p>贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。</p>
</li>
<li><p>MITIE方法，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。</p>
</li>
<li><p>最新提出的LIPO的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。</p>
</li>
</ol>
<h3 id="3-3-超参数搜索一般过程？"><a href="#3-3-超参数搜索一般过程？" class="headerlink" title="3.3 超参数搜索一般过程？"></a>3.3 超参数搜索一般过程？</h3><p>超参数搜索一般过程：</p>
<ol>
<li>将数据集划分成训练集、验证集及测试集。</li>
<li>在训练集上根据模型的性能指标对模型参数进行优化。</li>
<li>在验证集上根据模型的性能指标对模型的超参数进行搜索。</li>
<li>步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。</li>
</ol>
<p>其中，搜索过程需要搜索算法，一般有：网格搜索、随机搜过、启发式智能搜索、贝叶斯搜索。</p>
<h2 id="4-激活函数"><a href="#4-激活函数" class="headerlink" title="4 激活函数"></a>4 激活函数</h2><h3 id="4-1-为什么需要非线性激活函数？"><a href="#4-1-为什么需要非线性激活函数？" class="headerlink" title="4.1 为什么需要非线性激活函数？"></a>4.1 为什么需要非线性激活函数？</h3><p><strong>为什么需要激活函数？</strong></p>
<ol>
<li>激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。</li>
<li>激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。</li>
<li>激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</li>
</ol>
<p><strong>为什么激活函数需要非线性函数？</strong></p>
<ol>
<li>假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。</li>
<li>使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。</li>
</ol>
<h3 id="4-2-常见的激活函数及图像"><a href="#4-2-常见的激活函数及图像" class="headerlink" title="4.2 常见的激活函数及图像"></a>4.2 常见的激活函数及图像</h3><ol>
<li><h4 id="sigmoid-激活函数"><a href="#sigmoid-激活函数" class="headerlink" title="sigmoid 激活函数"></a>sigmoid 激活函数</h4><p>函数的定义为：$ f(x) = \frac{1}{1 + e^{-x}} $，其值域为 $ (0,1) $。</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="https://s2.loli.net/2023/09/25/AgZQu726rcWSXMh.png" alt=""></p>
<ol>
<li><h4 id="tanh激活函数"><a href="#tanh激活函数" class="headerlink" title="tanh激活函数"></a>tanh激活函数</h4><p>函数的定义为：$ f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $，值域为 $ (-1,1) $。</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="https://s2.loli.net/2023/09/25/KYTDWEnSGCehvNd.png" alt=""></p>
<ol>
<li><h4 id="Relu激活函数"><a href="#Relu激活函数" class="headerlink" title="Relu激活函数"></a>Relu激活函数</h4><p>函数的定义为：$ f(x) = max(0, x) $  ，值域为 $ [0,+∞) $；</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="https://s2.loli.net/2023/09/25/PG7wbjtyr84m3Kz.png" alt=""></p>
<ol>
<li><h4 id="Leak-Relu-激活函数"><a href="#Leak-Relu-激活函数" class="headerlink" title="Leak Relu 激活函数"></a>Leak Relu 激活函数</h4><p>函数定义为： $ f(x) =  \left\{<br>\begin{aligned}<br>ax, \quad x<0 \\
x, \quad x>0<br>\end{aligned}<br>\right. $，值域为 $ (-∞,+∞) $。 </p>
<p>图像如下（$ a = 0.5 $）：</p>
</li>
</ol>
<p><img src="https://s2.loli.net/2023/09/25/mWzuSy2wirGo38E.png" alt=""></p>
<ol>
<li><h4 id="SoftPlus-激活函数"><a href="#SoftPlus-激活函数" class="headerlink" title="SoftPlus 激活函数"></a>SoftPlus 激活函数</h4><p>函数的定义为：$ f(x) = ln( 1 + e^x) $，值域为 $ (0,+∞) $。</p>
<p>函数图像如下:</p>
</li>
</ol>
<p><img src="https://s2.loli.net/2023/09/25/S6ko349swDvjiZh.png" alt=""></p>
<ol>
<li><h4 id="softmax-函数"><a href="#softmax-函数" class="headerlink" title="softmax 函数"></a>softmax 函数</h4><p>函数定义为： $ \sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} $。</p>
<p>Softmax 多用于多分类神经网络输出。</p>
</li>
</ol>
<h3 id="4-3-常见激活函数的导数计算？"><a href="#4-3-常见激活函数的导数计算？" class="headerlink" title="4.3 常见激活函数的导数计算？"></a>4.3 常见激活函数的导数计算？</h3><p>对常见激活函数，导数计算如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原函数</th>
<th>函数表达式</th>
<th>导数</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid激活函数</td>
<td>$f(x)=\frac{1}{1+e^{-x}}$</td>
<td>$f^{‘}(x)=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))$</td>
<td>当$x=10$,或$x=-10$，$f^{‘}(x) \approx0$,当$x=0$$f^{‘}(x) =0.25$</td>
</tr>
<tr>
<td>Tanh激活函数</td>
<td>$f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$</td>
<td>$f^{‘}(x)=-(tanh(x))^2$</td>
<td>当$x=10$,或$x=-10$，$f^{‘}(x) \approx0$,当$x=0$$f^{`}(x) =1$</td>
</tr>
<tr>
<td>Relu激活函数</td>
<td>$f(x)=max(0,x)$</td>
<td>$c(u)=\begin{cases} 0,x<0 \\ 1,x>0 \ undefined,x=0\end{cases}$</td>
<td>通常$x=0$时，给定其导数为1和0</td>
</tr>
</tbody>
</table>
</div>
<h3 id="4-4-激活函数有哪些性质？"><a href="#4-4-激活函数有哪些性质？" class="headerlink" title="4.4 激活函数有哪些性质？"></a>4.4 激活函数有哪些性质？</h3><ol>
<li>非线性： 当激活函数是非线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 $ f(x)=x $，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；</li>
<li>可微性： 当优化方法是基于梯度的时候，就体现了该性质；</li>
<li>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；</li>
<li>$ f(x)≈x $： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；</li>
<li>输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的 Learning Rate。</li>
</ol>
<h3 id="4-5-如何选择激活函数？"><a href="#4-5-如何选择激活函数？" class="headerlink" title="4.5 如何选择激活函数？"></a>4.5 如何选择激活函数？</h3><p>​    选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<p>以下是常见的选择情况：</p>
<ol>
<li>如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。</li>
<li>如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。</li>
<li>sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。</li>
<li>tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。</li>
<li>ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。</li>
<li>如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。</li>
</ol>
<h3 id="4-6-使用-ReLu-激活函数的优点？"><a href="#4-6-使用-ReLu-激活函数的优点？" class="headerlink" title="4.6 使用 ReLu 激活函数的优点？"></a>4.6 使用 ReLu 激活函数的优点？</h3><ol>
<li>在区间变动很大的情况下，ReLu 激活函数的导数或者激活函数的斜率都会远大于 0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。</li>
<li>sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。</li>
<li>需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会产生这个问题。</li>
</ol>
<h3 id="4-7-什么时候可以用线性激活函数？"><a href="#4-7-什么时候可以用线性激活函数？" class="headerlink" title="4.7 什么时候可以用线性激活函数？"></a>4.7 什么时候可以用线性激活函数？</h3><ol>
<li>输出层，大多使用线性激活函数。</li>
<li>在隐含层可能会使用一些线性激活函数。</li>
<li>一般用到的线性激活函数很少。</li>
</ol>
<h3 id="4-8-怎样理解-Relu（-lt-0-时）是非线性激活函数？"><a href="#4-8-怎样理解-Relu（-lt-0-时）是非线性激活函数？" class="headerlink" title="4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？"></a>4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？</h3><p>Relu 激活函数图像如下：</p>
<p><img src="https://s2.loli.net/2023/09/25/PG7wbjtyr84m3Kz.png" alt=""></p>
<p>根据图像可看出具有如下特点：</p>
<ol>
<li><p>单侧抑制；</p>
</li>
<li><p>相对宽阔的兴奋边界；</p>
</li>
<li><p>稀疏激活性；</p>
<p>ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。</p>
<p>因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</p>
<p><strong>稀疏激活性</strong>：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $ x<0 $ 时，ReLU 硬饱和，而当 $ x>0 $ 时，则不存在饱和问题。ReLU 能够在 $ x&gt;0 $ 时保持梯度不衰减，从而缓解梯度消失问题。</p>
</li>
</ol>
<h3 id="4-9-Softmax-定义及作用"><a href="#4-9-Softmax-定义及作用" class="headerlink" title="4.9 Softmax 定义及作用"></a>4.9 Softmax 定义及作用</h3><p>Softmax 是一种形如下式的函数：</p>
<script type="math/tex; mode=display">
P(i) = \frac{exp(\theta_i^T x)}{\sum_{k=1}^{K} exp(\theta_i^T x)}</script><p>​    其中，$ \theta_i $ 和 $ x $ 是列向量，$ \theta_i^T x $ 可能被换成函数关于 $ x $ 的函数 $ f_i(x) $</p>
<p>​    通过 softmax 函数，可以使得 $ P(i) $ 的范围在 $ [0,1] $ 之间。在回归和分类问题中，通常 $ \theta $ 是待求参数，通过寻找使得 $ P(i) $ 最大的 $ \theta_i $ 作为最佳参数。</p>
<p>​    但是，使得范围在 $ [0,1] $  之间的方法有很多，为啥要在前面加上以 $ e $ 的幂函数的形式呢？参考 logistic 函数：</p>
<script type="math/tex; mode=display">
P(i) = \frac{1}{1+exp(-\theta_i^T x)}</script><p>​    这个函数的作用就是使得 $ P(i) $ 在负无穷到 0 的区间趋向于 0， 在 0 到正无穷的区间趋向 1,。同样 softmax 函数加入了 $ e $ 的幂函数正是为了两极化：正样本的结果将趋近于 1，而负样本的结果趋近于 0。这样为多类别提供了方便（可以把 $ P(i) $ 看做是样本属于类别的概率）。可以说，Softmax 函数是 logistic 函数的一种泛化。</p>
<p>​    softmax 函数可以把它的输入，通常被称为 logits 或者 logit scores，处理成 0 到 1 之间，并且能够把输出归一化到和为 1。这意味着 softmax 函数与分类的概率分布等价。它是一个网络预测多酚类问题的最佳输出激活函数。</p>
<h3 id="4-10-Softmax-函数如何应用于多分类？"><a href="#4-10-Softmax-函数如何应用于多分类？" class="headerlink" title="4.10 Softmax 函数如何应用于多分类？"></a>4.10 Softmax 函数如何应用于多分类？</h3><p>​    softmax 用于多分类过程中，它将多个神经元的输出，映射到 $ (0,1) $ 区间内，可以看成概率来理解，从而来进行多分类！</p>
<p>​    假设我们有一个数组，$ V_i $ 表示 $ V $  中的第 $ i $ 个元素，那么这个元素的 softmax 值就是</p>
<script type="math/tex; mode=display">
S_i = \frac{e^{V_i}}{\sum_j e^{V_j}}</script><p>​    从下图看，神经网络中包含了输入层，然后通过两个特征层处理，最后通过 softmax 分析器就能得到不同条件下的概率，这里需要分成三个类别，最终会得到 $ y=0, y=1, y=2 $ 的概率值。</p>
<p><img src="J:/DeepLearning-500-questions-master/ch03_深度学习基础/img/ch3/3.4.9.1.png" alt=""></p>
<p>继续看下面的图，三个输入通过 softmax 后得到一个数组 $ [0.05 , 0.10 , 0.85] $，这就是 soft 的功能。</p>
<p><img src="https://s2.loli.net/2023/09/25/75CRBLgPhAvItVY.png" alt=""></p>
<p>更形象的映射过程如下图所示：</p>
<p><img src="J:/DeepLearning-500-questions-master/ch03_深度学习基础/img/ch3/3.4.9.3.png" alt="****"></p>
<p>​    softmax 直白来说就是将原来输出是 $ 3,1,-3 $ 通过 softmax 函数一作用，就映射成为 $ (0,1) $ 的值，而这些值的累和为 $ 1 $（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！</p>
<h3 id="4-11-交叉熵代价函数定义及其求导推导"><a href="#4-11-交叉熵代价函数定义及其求导推导" class="headerlink" title="4.11 交叉熵代价函数定义及其求导推导"></a>4.11 交叉熵代价函数定义及其求导推导</h3><p>(<strong>贡献者：黄钦建－华南理工大学</strong>)</p>
<p>​    神经元的输出就是 a = σ(z)，其中$z=\sum w_{j}i_{j}+b$是输⼊的带权和。</p>
<p>$C=-\frac{1}{n}\sum[ylna+(1-y)ln(1-a)]$</p>
<p>​    其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的， y 是对应的⽬标输出。</p>
<p>​    表达式是否解决学习缓慢的问题并不明显。实际上，甚⾄将这个定义看做是代价函数也不是显⽽易⻅的！在解决学习缓慢前，我们来看看交叉熵为何能够解释成⼀个代价函数。</p>
<p>​    将交叉熵看做是代价函数有两点原因。</p>
<p>​    第⼀，它是⾮负的， C &gt; 0。可以看出：式子中的求和中的所有独⽴的项都是负数的，因为对数函数的定义域是 (0，1)，并且求和前⾯有⼀个负号，所以结果是非负。</p>
<p>​    第⼆，如果对于所有的训练输⼊ x，神经元实际的输出接近⽬标值，那么交叉熵将接近 0。</p>
<p>​    假设在这个例⼦中， y = 0 ⽽ a ≈ 0。这是我们想到得到的结果。我们看到公式中第⼀个项就消去了，因为 y = 0，⽽第⼆项实际上就是 − ln(1 − a) ≈ 0。反之， y = 1 ⽽ a ≈ 1。所以在实际输出和⽬标输出之间的差距越⼩，最终的交叉熵的值就越低了。（这里假设输出结果不是0，就是1，实际分类也是这样的）</p>
<p>​    综上所述，交叉熵是⾮负的，在神经元达到很好的正确率的时候会接近 0。这些其实就是我们想要的代价函数的特性。其实这些特性也是⼆次代价函数具备的。所以，交叉熵就是很好的选择了。但是交叉熵代价函数有⼀个⽐⼆次代价函数更好的特性就是它避免了学习速度下降的问题。为了弄清楚这个情况，我们来算算交叉熵函数关于权重的偏导数。我们将$a={\varsigma}(z)$代⼊到 公式中应⽤两次链式法则，得到：</p>
<p>$\begin{eqnarray}\frac{\partial C}{\partial w_{j}}&amp;=&amp;-\frac{1}{n}\sum \frac{\partial }{\partial w_{j}}[ylna+(1-y)ln(1-a)]\\&amp;=&amp;-\frac{1}{n}\sum \frac{\partial }{\partial a}[ylna+(1-y)ln(1-a)]<em>\frac{\partial a}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{a}-\frac{1-y}{1-a})</em>\frac{\partial a}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)})\frac{\partial \varsigma(z)}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)}){\varsigma}’(z)x_{j}\end{eqnarray}$</p>
<p>​    根据$\varsigma(z)=\frac{1}{1+e^{-z}}$ 的定义，和⼀些运算，我们可以得到 ${\varsigma}’(z)=\varsigma(z)(1-\varsigma(z))$。化简后可得：</p>
<p>$\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum x_{j}({\varsigma}(z)-y)$</p>
<p>​    这是⼀个优美的公式。它告诉我们权重学习的速度受到$\varsigma(z)-y$，也就是输出中的误差的控制。更⼤的误差，更快的学习速度。这是我们直觉上期待的结果。特别地，这个代价函数还避免了像在⼆次代价函数中类似⽅程中${\varsigma}’(z)$导致的学习缓慢。当我们使⽤交叉熵的时候，${\varsigma}’(z)$被约掉了，所以我们不再需要关⼼它是不是变得很⼩。这种约除就是交叉熵带来的特效。实际上，这也并不是⾮常奇迹的事情。我们在后⾯可以看到，交叉熵其实只是满⾜这种特性的⼀种选择罢了。</p>
<p>​    根据类似的⽅法，我们可以计算出关于偏置的偏导数。我这⾥不再给出详细的过程，你可以轻易验证得到：</p>
<p>$\frac{\partial C}{\partial b}=\frac{1}{n}\sum ({\varsigma}(z)-y)$</p>
<p>​    再⼀次, 这避免了⼆次代价函数中类似${\varsigma}’(z)$项导致的学习缓慢。</p>
<h3 id="4-12-为什么Tanh收敛速度比Sigmoid快？"><a href="#4-12-为什么Tanh收敛速度比Sigmoid快？" class="headerlink" title="4.12 为什么Tanh收敛速度比Sigmoid快？"></a>4.12 为什么Tanh收敛速度比Sigmoid快？</h3><p>首先看如下两个函数的求导：</p>
<p>$tanh^{,}(x)=1-tanh(x)^{2}\in (0,1)$</p>
<p>$s^{,}(x)=s(x)*(1-s(x))\in (0,\frac{1}{4}]$</p>
<p>由上面两个公式可知tanh(x)梯度消失的问题比sigmoid轻，所以Tanh收敛速度比Sigmoid快。</p>
<p>注：梯度消失（gradient vanishing）或者爆炸（gradient explosion）是激活函数<strong>以及当前权重</strong>耦合产生的综合结果：<br>​   设任意激活函数为$\sigma(\cdot)$，k+1层网络输出为$f_{k+1}=\sigma(Wf_k)$，求导得到$\frac {\partial h_{t+1}}{\partial h_t}=diag(\sigma’(Wh_t))W$。可见求导结果同时会受到权重$W$和激活函数的导数$\sigma’(\cdot)$的影响，以sigmoid函数$\sigma(X)=\frac {1}{1+e^{-x}}$为例，其导数为$\sigma’(x)=\frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}})$，其值恒大于零小于1，用链式法则求梯度回传时连续相乘使得结果趋于0，但是如果权重$W$是较大的数值，使得$\frac {\partial f_{t+1}}{\partial f_t}$相乘结果大于1，则梯度回传时连续相乘则不会发生梯度消失。<br>综上，在讨论激活函数收敛速度或与梯度消失或者爆炸相关时，应同时考虑当前权重$W$数值的影响。</p>
<h3 id="4-13-内聚外斥-Center-Loss"><a href="#4-13-内聚外斥-Center-Loss" class="headerlink" title="4.13 内聚外斥 - Center Loss"></a>4.13 内聚外斥 - Center Loss</h3><p>在计算机视觉任务中, 由于其简易性, 良好的表现, 与对分类任务的概率性理解, Cross Entropy Loss (交叉熵代价) + Softmax 组合被广泛应用于以分类任务为代表的任务中. 在此应用下, 我们可将其学习过程进一步理解为: 更相似(同类/同物体)的图像在特征域中拥有“更近的距离”, 相反则”距离更远“. 换而言之, 我们可以进一步理解为其学习了一种低类内距离(Intra-class Distance)与高类间距离(Inter-class Distance)的特征判别模型. 在此Center Loss则可以高效的计算出这种具判别性的特征. 不同于传统的Softmax Loss, Center Loss通过学习“特征中心”从而最小化其类内距离. 其表达形式如下:</p>
<p>$L_{C} = \frac{1}{2}\sum^{m}_{i=1}||x_{i}-c_{y_{i}}||^{2}_{2}$</p>
<p>其中$x_{i}$表示FCN(全连接层)之前的特征, $c_{y_{i}}$表示第$y_{i} $个类别的特征中心, $m$表示mini-batch的大小. 我们很清楚的看到$L_{C}$的终极目标为最小化每个特征与其特征中心的方差, 即最小化类内距离. 其迭代公式为:</p>
<p>$\frac{\partial L_{C}}{\partial x_{i}}=x_{i}-c_{y_{i}}$</p>
<p>$\Delta{c_{j}} = \frac{\sum^{m}_{i=1}\delta(y_{i}=j)\cdot(c_{j}-x_{i})}{1+\sum^{m}_{i=1}\delta(y_{i}=j)}$</p>
<p>其中$ \delta(condition)=\left\{<br>\begin{array}{rcl}<br>1       &amp;      &amp; {condition is True}\\<br>0     &amp;      &amp; {otherwise}\ \end{array} \right.$</p>
<p>结合Softmax, 我们可以搭配二者使用, 适当平衡这两种监督信号. 在Softmax拉开类间距离的同时, 利用Center Loss最小化类内距离. 例如:</p>
<p>$\begin{eqnarray}L &amp; = &amp; L_{S} + \lambda L_{C} \ &amp;=&amp; -\sum^{m}_{i=1}log\frac{e^{W_{y}^{T}x_{i}+b_{y_{i}}}}{\sum^{m}_{i=1}e^{W^{T}_{j}x_{i}+b_{j}}} + \frac{\lambda}{2}\sum^{m}_{i=1}||x_{i}-c_{y_{i}}||^{2}_{2}\ \end{eqnarray}$</p>
<p>即便如此, Center Loss仍有它的不足之处: 其特征中心为存储在网络模型之外的额外参数, 不能与模型参数一同优化. 这些额外参数将与记录每一步特征变化的自动回归均值估计(autoregressive mean estimator)进行更迭. 当需要学习的类别数量较大时, mini-batch可能无力提供足够的样本进行均值估计. 若此Center Loss将需要平衡两种监督损失来以确定更迭, 其过程需要一个对平衡超参数的搜索过程, 使得其择值消耗昂贵.</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Silas</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://example.com/2023/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%80/">http://example.com/2023/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%80/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Silas</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">深度学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2023/02/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BA%8C/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/1.jpg" class="responsive-img" alt="深度学习&lt;二&gt;">
                        
                        <span class="card-title">深度学习&lt;二&gt;</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2023-02-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    深度学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">深度学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/01/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%89/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/9.jpg" class="responsive-img" alt="机器学习&lt;三&gt;">
                        
                        <span class="card-title">机器学习&lt;三&gt;</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-01-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    机器学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/">
                        <span class="chip bg-color">常见算法</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2022-2023</span>
            
            <a href="/about" target="_blank">Silas</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/blinkfox" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1181062873@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1181062873" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1181062873" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
